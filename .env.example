# Circuit Breaker Environment Configuration
# Copy this file to .env and fill in your actual values

# =============================================================================
# SERVER CONFIGURATION
# =============================================================================

# Server host and port
SERVER_HOST=localhost
SERVER_PORT=4000

# GraphQL endpoint configuration
GRAPHQL_ENDPOINT=http://localhost:4000/graphql
GRAPHQL_WS_ENDPOINT=ws://localhost:4000/graphql

# Server environment (development, staging, production)
ENVIRONMENT=development

# Logging level (trace, debug, info, warn, error)
LOG_LEVEL=info

# =============================================================================
# AI AGENT LLM PROVIDERS
# =============================================================================

# Primary Provider: Anthropic Configuration (recommended)
ANTHROPIC_API_KEY=your_anthropic_api_key_here
ANTHROPIC_BASE_URL=https://api.anthropic.com
ANTHROPIC_DEFAULT_MODEL=claude-3-5-sonnet-20241022

# Alternative Providers (uncomment to use):

# OpenAI Configuration
# OPENAI_API_KEY=your_openai_api_key_here
# OPENAI_BASE_URL=https://api.openai.com/v1
# OPENAI_DEFAULT_MODEL=gpt-4

# Google Gemini Configuration
# GOOGLE_API_KEY=your_google_api_key_here
# GOOGLE_BASE_URL=https://generativelanguage.googleapis.com/v1
# GOOGLE_DEFAULT_MODEL=gemini-pro

# Ollama Configuration (for local models)
# OLLAMA_BASE_URL=http://localhost:11434
# OLLAMA_DEFAULT_MODEL=llama2

# vLLM Configuration (for high-performance inference)
#
# For EC2 GPU instances (recommended for production):
# 1. Run: ./setup/setup_vllm_ec2.sh
# 2. Replace 'your-ec2-instance-ip' with actual EC2 public IP
# 3. Uncomment the lines below:
# VLLM_BASE_URL=http://your-ec2-instance-ip:8000/v1
# VLLM_API_KEY=
# VLLM_DEFAULT_MODEL=meta-llama/Llama-2-7b-chat-hf
# VLLM_TIMEOUT=300
# VLLM_MAX_TOKENS=2048
# VLLM_GPU_MEMORY_UTILIZATION=0.9
# VLLM_TENSOR_PARALLEL_SIZE=1
# VLLM_VERIFY_SSL=false
#
# For local GPU setup (if you have NVIDIA GPU):
# VLLM_BASE_URL=http://localhost:8000/v1
# VLLM_VERIFY_SSL=true
#
# Example EC2 configuration after setup:
# VLLM_BASE_URL=http://54.123.45.67:8000/v1
# VLLM_DEFAULT_MODEL=meta-llama/Llama-2-7b-chat-hf
# VLLM_TIMEOUT=300

# Custom LLM Provider Configuration
# CUSTOM_LLM_ENDPOINT=https://your-custom-llm-endpoint.com/v1
# CUSTOM_LLM_API_KEY=your_custom_api_key_here
# CUSTOM_LLM_MODEL=your_custom_model_name

# =============================================================================
# AGENT ENGINE CONFIGURATION
# =============================================================================

# Maximum concurrent agent executions
AGENT_MAX_CONCURRENT_EXECUTIONS=50

# Stream buffer size for real-time events
AGENT_STREAM_BUFFER_SIZE=1000

# Connection timeout in seconds
AGENT_CONNECTION_TIMEOUT=30

# Agent execution timeout in seconds
AGENT_EXECUTION_TIMEOUT=300

# Cleanup interval in seconds
AGENT_CLEANUP_INTERVAL=60

# Default retry configuration
AGENT_DEFAULT_MAX_ATTEMPTS=3
AGENT_DEFAULT_BACKOFF_SECONDS=10

# Agent-specific configuration examples
AGENT_CLASSIFICATION_TEMPERATURE=0.1
AGENT_CLASSIFICATION_MAX_TOKENS=100
AGENT_REVIEW_TEMPERATURE=0.3
AGENT_REVIEW_MAX_TOKENS=500
AGENT_INITIAL_DELAY_SECONDS=2

# =============================================================================
# STORAGE CONFIGURATION
# =============================================================================

# Storage backend (memory, postgresql, nats, redis)
STORAGE_BACKEND=memory

# PostgreSQL Configuration (if using PostgreSQL storage)
DATABASE_URL=postgresql://username:password@localhost:5432/circuit_breaker
POSTGRES_HOST=localhost
POSTGRES_PORT=5432
POSTGRES_USER=circuit_breaker
POSTGRES_PASSWORD=your_postgres_password
POSTGRES_DATABASE=circuit_breaker
POSTGRES_MAX_CONNECTIONS=10

# NATS Configuration (if using NATS storage)
NATS_URL=nats://localhost:4222
NATS_CLUSTER_ID=circuit-breaker-cluster
NATS_CLIENT_ID=circuit-breaker-client
NATS_JETSTREAM_DOMAIN=circuit-breaker

# Redis Configuration (if using Redis storage)
REDIS_URL=redis://localhost:6379
REDIS_HOST=localhost
REDIS_PORT=6379
REDIS_PASSWORD=your_redis_password
REDIS_DATABASE=0
REDIS_MAX_CONNECTIONS=10

# =============================================================================
# SECURITY CONFIGURATION
# =============================================================================

# JWT Configuration (if using authentication)
JWT_SECRET=your_jwt_secret_key_here
JWT_EXPIRATION=24h

# API Rate Limiting
RATE_LIMIT_REQUESTS_PER_MINUTE=100
RATE_LIMIT_BURST_SIZE=20

# CORS Configuration
CORS_ALLOWED_ORIGINS=http://localhost:3000,http://localhost:8080
CORS_ALLOW_CREDENTIALS=true

# TLS Configuration (for production)
TLS_CERT_PATH=/path/to/cert.pem
TLS_KEY_PATH=/path/to/key.pem

# =============================================================================
# OAUTH CONFIGURATION (for Remote MCP Server)
# =============================================================================

# Remote MCP Server OAuth Settings
MCP_OAUTH_ENABLED=true
MCP_OAUTH_DEFAULT_PROVIDER=github
MCP_OAUTH_CALLBACK_URL=http://localhost:8080/mcp/remote/oauth/callback

# GitHub OAuth Provider
GITHUB_OAUTH_CLIENT_ID=your_github_client_id_here
GITHUB_OAUTH_CLIENT_SECRET=your_github_client_secret_here
GITHUB_OAUTH_SCOPE=read:user,repo

# GitLab OAuth Provider (optional)
GITLAB_OAUTH_CLIENT_ID=your_gitlab_client_id_here
GITLAB_OAUTH_CLIENT_SECRET=your_gitlab_client_secret_here
GITLAB_OAUTH_SCOPE=read_user,api

# Google OAuth Provider (optional)
GOOGLE_OAUTH_CLIENT_ID=your_google_client_id_here
GOOGLE_OAUTH_CLIENT_SECRET=your_google_client_secret_here
GOOGLE_OAUTH_SCOPE=openid,profile,email

# =============================================================================
# FUNCTION RUNNER CONFIGURATION
# =============================================================================

# Docker configuration for function execution
DOCKER_HOST=unix:///var/run/docker.sock
DOCKER_API_VERSION=1.41

# Function execution limits
FUNCTION_MAX_MEMORY_MB=512
FUNCTION_MAX_CPU_CORES=1
FUNCTION_MAX_EXECUTION_TIME=300

# Function storage configuration
FUNCTION_STORAGE_PATH=./functions
FUNCTION_TEMP_PATH=/tmp/circuit-breaker-functions

# =============================================================================
# MONITORING AND OBSERVABILITY
# =============================================================================

# Metrics configuration
METRICS_ENABLED=true
METRICS_PORT=9090
METRICS_PATH=/metrics

# Tracing configuration
TRACING_ENABLED=false
JAEGER_ENDPOINT=http://localhost:14268/api/traces
TRACE_SAMPLE_RATE=0.1

# Health check configuration
HEALTH_CHECK_ENABLED=true
HEALTH_CHECK_PATH=/health

# =============================================================================
# DEVELOPMENT CONFIGURATION
# =============================================================================

# Enable development features
DEV_MODE=true
DEV_PRETTY_LOGS=true
DEV_GRAPHQL_PLAYGROUND=true
DEV_GRAPHQL_INTROSPECTION=true

# Hot reload configuration
HOT_RELOAD_ENABLED=true
HOT_RELOAD_WATCH_PATHS=./src,./examples

# Testing configuration
TEST_DATABASE_URL=postgresql://test:test@localhost:5432/circuit_breaker_test
TEST_TIMEOUT_SECONDS=30

# =============================================================================
# EXAMPLE CONFIGURATIONS
# =============================================================================

# Example workflow configuration
EXAMPLE_WORKFLOW_ENABLED=true
EXAMPLE_TOKENS_COUNT=10

# Demo agent configurations
DEMO_CLASSIFICATION_AGENT_ENABLED=true
DEMO_REVIEW_AGENT_ENABLED=true
DEMO_CONTENT_GENERATOR_ENABLED=false

# =============================================================================
# USAGE INSTRUCTIONS
# =============================================================================

# 1. Copy this file to .env:
#    cp .env.example .env
#
# 2. Fill in your actual API keys:
#    - Get Anthropic API key from: https://console.anthropic.com/ (primary)
#    - Get OpenAI API key from: https://platform.openai.com/api-keys (alternative)
#    - Get Google API key from: https://makersuite.google.com/app/apikey (alternative)
#    - Configure vLLM URL if using EC2 deployment (see setup/setup_vllm_ec2.sh)
#    - For EC2: Replace 'your-ec2-instance-ip' with actual EC2 public IP from setup script output
#    - For local GPU: Use http://localhost:8000/v1
#    - EC2 setup creates: ~/.config/circuit-breaker/vllm_ec2.env with your specific configuration
#
# 3. Adjust configuration values as needed for your use case
#
# 4. Keep your .env file secure and never commit it to version control
